---
title: "Sparse Clustering Overview"
author: "Paul Harmon"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  prettydoc::html_pretty:
    theme: tactile
    highlight: github
---

```{r, echo=FALSE}
htmltools::img(src = knitr::image_uri('msutrans.png'), 
               alt = 'logo', 
               style = 'position:absolute; top:1px; left:0px; padding:2px;height:100px;')

               
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2);library(dplyr);library(plotly)
library(tibble); library(DT); library(viridis)
library(ggpubr)
```

# Introduction
This document is designed to cover some of the high points of sparse clustering and its implementation in R using the sparcl. 

**A Note:** Sparcl package was removed from CRAN on 7/20/2018 "as check problems were not corrected despite reminders."(See page at https://cran.r-project.org/web/packages/sparcl/index.html.) However, there is another version out there by Kondo, Salibian-Bareera and Zamar (2016) called RSKC. 

# How does Sparse Clustering Work?
Sparse clustering works to combine both the **dimension reduction** and the **clustering** into a single step. While Tibshirani and Witten (2010) implemented the sparse clustering on both K-means and hierarchical clustering methods, the method that seems to have been improved the most is the k-means version, which Kondo et. al. (2016) made some nice changes to. 

The idea is simple: Add a weighting term $w_j$ to the objective function that is to be optimized (in this case, maximized) for each of the features in the dataset. Subject that weighting term to a lasso (L1-norm) penalty, allowing it to be set to 0 for certain features. This allows the algorithm to achieve sparsity (i.e. perform feature selection) and then perform clustering on those nonzero features. 

Kondo takes this a step further. The original sparse k-means version had two big problems: **missingness** and **outliers**. Basically, it could only be used on complete cases and tended to pick up signals from features that were not important but happened to have large outlier values. Kondo et al. solve this problem by using a method called **trimmed k-means**, which takes the farthest outlying $\alpha*100$ percent of the points from the cluster centers (at each iteration, I think) and removes them from the algorithm, then runs k-means. 



# Implementations in R: 

```{r, fig.align = 'center', message = FALSE}
#installs and libraries the RSKC package
#install.packages('RSKC')
suppressMessages(library(RSKC))

#installs and libraries the Sparcl package (possibly deprecated)
#install the most recent version then use R-studio to install from archived file
#install.packages("C:/Users/r74t532/Downloads/sparcl_1.0.3.tar.gz", repos = NULL, type = "source")
#devtools::install_version('sparcl',version = '1.0.3')
#library(sparcl) #works on my laptop but not OPA computer
```



#Datasets 

## NBA Rookies 2017
```{r}
nba <- read.csv('data/na.csv', header = TRUE)

#pca-based
pc1 <- prcomp(nba[,-c(1,2,3)], scale = TRUE, center = TRUE)
summary(pc1)

#do some clustering
library(mclust)
modclust <- mclustBIC(pc1$x[,1:2])
mc <- Mclust(pc1$x[,1:2], x = modclust)

#build a dataframe
dat1 <- tibble(pc1$x[,1],pc1$x[,2],mc$classification); names(dat1) <- c('s1','s2','class')

#build a plot
p <- ggplot(dat1) + geom_point(aes(s1,s2,color = factor(class))) + ggtitle("NBA Rookies 2017") + theme_classic()

ggplotly(p)


##################
##Compare the Sparse Clustering Methods to this: 
library(RSKC)
spk <- RSKC(nba[,-c(1,2,3)], ncl = 3, alpha = 0, L1 = 1) # Sparse K-Means
#see documentation but alpha = 0 and l1 = 1 gives sparse K means
#to get "robust" sparse k-means, we need alpha >0 and L1 = 1

rspk <- RSKC(nba[,-c(1,2,3)], ncl = 3, alpha = .5, L1 = 1) # Robust Sparse K-Means

dat1$spk <- spk$labels
dat1$rspk <- rspk$labels

#gives the sparse k-means 
p2 <- ggplot(dat1) + geom_point(aes(s1,s2,color = factor(spk))) + ggtitle("NBA Rookies 2017") + theme_classic()
ggplotly(p2)


#gives the robust sparse k-means
p3 <- ggplot(dat1) + geom_point(aes(s1,s2,color = factor(rspk))) + ggtitle("NBA Rookies 2017") + theme_classic()
ggplotly(p3)

```

Additionally, the 'sparcl' package can be used to perform sparse clustering (although it might not be available as it is currently not on CRAN and must be installed from an archive). Insofar as I know, it is the only implementation for sparse hierarchical clustering. 

```{r, eval = FALSE}
#sparcl package as well
library(sparcl)
KMeansSparseCluster(nba[,-c(1,2,3)], K = 3, wbounds = 2)
#higher penalty on weights
KMeansSparseCluster(nba[,-c(1,2,3)], K = 3, wbounds = 2400)
```





###Assessment of Wine Data

```{r, eval = FALSE}
wine <- read.csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', header = FALSE)
names(wine) <- c("Class","Alcohol",'Malic Acid','Ash','Alcalinity of ash','Magnesium','Total phenols',
                 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins','Color Intensity',
                 'Hue','0d280/od315 of diluted wines','Proline')
head(wine)

#typical kmeans
km.wine <- kmeans(wine, centers = 3)
#sparse kmeans
sparse.wine <- RSKC(wine, ncl = 3, alpha = 0, L1 = 1)
#robust kmeans
robust.wine <- RSKC(wine, ncl = 3, alpha = 0.5, L1 = 1)

##look a bit at the 


##compare cluster membership
clusters <- tibble(wine$Class,km.wine$cluster,sparse.wine$labels, robust.wine$labels)
datatable(clusters, options = list(pageLength = 5, scrollX = 75))


```



# Looking at the Carnegie Classifications
The Carnegie Classifications are calculated using 2 PCAs that form an aggregate and per-capita index. Then, clustering could be performed on the resulting scores (in the actual version, schools are subjectively grouped by their location relative to two arcs that split the plot into three groups). 

Using the seven variables in the Carnegie Classifications, we can obtain a similar clustering solution without the initial dimension reduction step: 


```{r, fig.align = 'center'}
##########
cc2015 <- dplyr::filter(read.csv("data/CC2015data.csv", header = TRUE, as.is = TRUE), BASIC2015 %in% c(15,16,17))
cc2015Ps<-
  na.omit(cc2015[,c("NAME","BASIC2010","BASIC2015","FACNUM","HUM_RSD","OTHER_RSD","SOCSC_RSD","STEM_RSD","PDNFRSTAFF","S.ER.D","NONS.ER.D")])

#calculate the ranked data
minrank <- function(x){rank(x, ties.method = "min")}
cc2015.r <- data.frame(cc2015Ps[,1:3],sapply(cc2015Ps[,-c(1:3)],minrank)) 
cc2015percap <- cc2015Ps[,c("PDNFRSTAFF","S.ER.D","NONS.ER.D")]/cc2015Ps$FACNUM
cc2015percap.r<-data.frame(sapply(cc2015percap,minrank))


#apply sparse clustering
clust <- RSKC(cc2015.r[,-c(1:3)], ncl = 3, alpha = .2)
clust$labels
clustUR <- RSKC(cc2015Ps[,-c(1,3)], ncl = 3, alpha = .2)
clustUR$labels

#table of clusters
table(clust$labels, cc2015.r$BASIC2015)
clust$labels[which(cc2015.r$NAME=="Montana State University")]    

#replicate PCA
pc.2015.rank <- prcomp(cc2015.r[,-c(1:4)], scale = FALSE)
pc.2015.percap <- prcomp(cc2015percap.r, scale = TRUE)
```


Interestingly, you get much of the same behavior as was present when we tried doing the K-means clustering on the two indices - at least given the categories of the axes, the clustering is not remarkably different from what the Carnegie Classifications would obtain.  The clustering is not aligned with the y=x line in the way that we might like, but that is to be expected because the sparse K-means is looking at the data as an aggregate, not at the two correlated indices used in the Carnegie Classifications. 

```{r, fig.align = 'center', fig.width = 13, fig.height = 4}
#create df
df1 <- tibble(pc.2015.rank$x[,1], pc.2015.percap$x[,1], cc2015Ps$BASIC2015, cc2015Ps$NAME)
names(df1) <- c("AG","PC","BASIC","NAME")
ggplot(df1) + geom_point(aes(AG,-PC, color = factor(BASIC))) + scale_color_viridis(option = "D",discrete = TRUE) + theme_classic() + ggtitle("Carnegie Classifications") + ylab("Per Capita") + xlab("Aggregate")

##Add classifications from RSKC
df1$SK <- clust$labels
df1$SKUR <- clustUR$labels
p1 <- ggplot(df1) + geom_point(aes(AG,-PC, color = factor(SK))) + scale_color_viridis(option = "D",discrete = TRUE) + theme_classic() + ggtitle("RSKC on Ranked") + ylab("Per Capita") + xlab("Aggregate")

p2 <- ggplot(df1) + geom_point(aes(AG,-PC, color = factor(SKUR))) + scale_color_viridis(option = "D",discrete = TRUE) + theme_classic() + ggtitle("RSKC on Raw Data") + ylab("Per Capita") + xlab("Aggregate")

ggarrange(p1,p2)

```


## Different Trimming Proportions

The plots below give the results on the sparse clustering on the raw data given different trimming proportions. 

```{r, fig.align = 'center'}
alpha <- c(0,5,10,20,50)
plot_list <- list()
for(j in 1:length(alpha)){
  
  clustUR <- RSKC(cc2015Ps[,-c(1,3)], ncl = 3, alpha = .2)
  df1$SKUR <- clustUR$labels
  p2 <- ggplot(df1) + geom_point(aes(AG,-PC, color = factor(SKUR))) + scale_color_viridis(option = "D",discrete = TRUE) + theme_classic() + ggtitle(paste("Raw with ",alpha[j]," Percent Trimmed")) + ylab("Per Capita") + xlab("Aggregate")
  plot_list[[j]] <- p2
}
#lapply(plot_list, print)#how to remove this garbage output?
plot_list
```





# Some Key Questions:

With PCA-based clustering, the visualization is pretty intuitive. You can show the clusters with a plot of the first two principal components (I suppose this would look pretty good even if you clustered on more than the first two PCs). However, this does not achieve variable selection/sparsity. 
When using sparse clustering, what are the best ways to visualize? Should I look at the features that were selected and then use plots of them to demonstrate clusters? 









